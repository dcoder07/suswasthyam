"""
AI model implementation for health prediction based on vital signs.
"""

import os
import json
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, LSTM, GRU, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import config

# Import ModelStub for fallback
try:
    from models.model_stub import ModelStub
except ImportError:
    # Define inline if import fails
    class ModelStub:
        def predict(self, X):
            batch_size = X.shape[0] if hasattr(X, 'shape') else 1
            return np.array([[0.3]] * batch_size)

class HealthPredictionModel:
    """Model for predicting health outcomes based on vital signs."""
    
    def __init__(self, input_shape=None, model_type=None):
        """
        Initialize the health prediction model.
        
        Args:
            input_shape (tuple): Shape of input data (time_steps, n_features)
            model_type (str): Type of model to use ("lstm", "gru", or "mlp")
        """
        self.model = None
        self.history = {"training": [], "fine_tuning": []}
        self.is_stub = False
        
        if input_shape is not None:
            self.build_model(input_shape, model_type)
    
    def build_model(self, input_shape, model_type=None):
        """
        Build the neural network model.
        
        Args:
            input_shape (tuple): Shape of input data (time_steps, n_features)
            model_type (str): Type of model to use
        """
        if model_type is None:
            model_type = config.MODEL_TYPE
            
        model = Sequential()
        
        if model_type.lower() == "lstm":
            # LSTM-based model for time series
            model.add(LSTM(config.HIDDEN_UNITS[0], return_sequences=True, 
                          input_shape=input_shape))
            model.add(BatchNormalization())
            model.add(Dropout(config.DROPOUT_RATE))
            
            model.add(LSTM(config.HIDDEN_UNITS[1]))
            model.add(BatchNormalization())
            model.add(Dropout(config.DROPOUT_RATE))
            
        elif model_type.lower() == "gru":
            # GRU-based model for time series
            model.add(GRU(config.HIDDEN_UNITS[0], return_sequences=True, 
                         input_shape=input_shape))
            model.add(BatchNormalization())
            model.add(Dropout(config.DROPOUT_RATE))
            
            model.add(GRU(config.HIDDEN_UNITS[1]))
            model.add(BatchNormalization())
            model.add(Dropout(config.DROPOUT_RATE))
            
        elif model_type.lower() == "mlp":
            # Simple MLP model
            model.add(tf.keras.layers.Flatten(input_shape=input_shape))
            model.add(Dense(config.HIDDEN_UNITS[0], activation='relu'))
            model.add(BatchNormalization())
            model.add(Dropout(config.DROPOUT_RATE))
            
            model.add(Dense(config.HIDDEN_UNITS[1], activation='relu'))
            model.add(BatchNormalization())
            model.add(Dropout(config.DROPOUT_RATE))
            
        else:
            raise ValueError(f"Unsupported model type: {model_type}")
        
        # Output layer - binary classification (0 = healthy, 1 = potential health issue)
        model.add(Dense(1, activation='sigmoid'))
        
        # Compile model
        model.compile(
            optimizer=Adam(learning_rate=config.LEARNING_RATE),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        self.model = model
        
        return model
    
    def train(self, X_train, y_train, X_val, y_val, epochs=None, batch_size=None):
        """
        Train the model on the provided data.
        
        Args:
            X_train (np.ndarray): Training features
            y_train (np.ndarray): Training targets
            X_val (np.ndarray): Validation features
            y_val (np.ndarray): Validation targets
            epochs (int): Number of training epochs
            batch_size (int): Batch size for training
            
        Returns:
            dict: Training history
        """
        if self.model is None:
            input_shape = (X_train.shape[1], X_train.shape[2])
            self.build_model(input_shape)
            
        if epochs is None:
            epochs = config.EPOCHS
            
        if batch_size is None:
            batch_size = config.BATCH_SIZE
            
        # Define callbacks
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=config.EARLY_STOPPING_PATIENCE,
                restore_best_weights=True
            ),
            ModelCheckpoint(
                config.MODEL_SAVE_PATH,
                monitor='val_loss',
                save_best_only=True,
                verbose=1
            )
        ]
        
        # Train the model
        history = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        # Save training history
        self.history["training"].append(history.history)
        self._save_history()
        
        return history.history
    
    def fine_tune(self, X_new, y_new, epochs=None, learning_rate=None):
        """
        Fine-tune the model with new data.
        
        Args:
            X_new (np.ndarray): New features for fine-tuning
            y_new (np.ndarray): New targets for fine-tuning
            epochs (int): Number of training epochs
            learning_rate (float): Learning rate for fine-tuning
            
        Returns:
            dict: Fine-tuning history
        """
        if self.model is None:
            raise ValueError("Model must be trained before fine-tuning")
            
        if epochs is None:
            epochs = config.FINE_TUNE_EPOCHS
            
        if learning_rate is None:
            learning_rate = config.FINE_TUNE_LEARNING_RATE
            
        # Update optimizer with lower learning rate
        self.model.compile(
            optimizer=Adam(learning_rate=learning_rate),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        # Fine-tune the model
        history = self.model.fit(
            X_new, y_new,
            epochs=epochs,
            batch_size=config.BATCH_SIZE,
            verbose=1
        )
        
        # Save fine-tuning history
        self.history["fine_tuning"].append(history.history)
        self._save_history()
        
        # Save updated model
        self.model.save(config.MODEL_SAVE_PATH)
        
        return history.history
    
    def predict(self, X):
        """
        Make predictions using the trained model.
        
        Args:
            X (np.ndarray): Input features
            
        Returns:
            np.ndarray: Predicted probabilities
        """
        if self.model is None:
            raise ValueError("Model must be trained before making predictions")
            
        return self.model.predict(X)
    
    def evaluate(self, X_test, y_test):
        """
        Evaluate the model on test data.
        
        Args:
            X_test (np.ndarray): Test features
            y_test (np.ndarray): Test targets
            
        Returns:
            dict: Evaluation metrics
        """
        if self.model is None:
            raise ValueError("Model must be trained before evaluation")
            
        # Get predictions
        y_pred_prob = self.model.predict(X_test)
        y_pred = (y_pred_prob > 0.5).astype(int).flatten()
        
        # Calculate metrics
        metrics = {
            "loss": self.model.evaluate(X_test, y_test, verbose=0)[0],
            "accuracy": accuracy_score(y_test, y_pred),
            "precision": precision_score(y_test, y_pred, zero_division=0),
            "recall": recall_score(y_test, y_pred, zero_division=0),
            "f1_score": f1_score(y_test, y_pred, zero_division=0),
            "roc_auc": roc_auc_score(y_test, y_pred_prob.flatten())
        }
        
        return metrics
    
    def save(self, filepath=None):
        """
        Save the model to disk.
        
        Args:
            filepath (str): Path to save the model
        """
        if self.model is None:
            raise ValueError("Cannot save an untrained model")
            
        if filepath is None:
            filepath = config.MODEL_SAVE_PATH
            
        self.model.save(filepath)
        self._save_history()
        
    def load(self, filepath=None):
        """
        Load a saved model from disk.
        
        Args:
            filepath (str): Path to the saved model
            
        Returns:
            bool: True if model loaded successfully
        """
        if filepath is None:
            filepath = config.MODEL_SAVE_PATH
            
        if not os.path.exists(filepath):
            print(f"Model file not found at {filepath}, using stub model...")
            # Use the model stub as a fallback
            self.model = ModelStub()
            self.is_stub = True
            return True
            
        try:
            self.model = load_model(filepath)
            self._load_history()
            return True
        except Exception as e:
            print(f"Error loading model: {str(e)}. Using stub model instead.")
            self.model = ModelStub()
            self.is_stub = True
            return True
    
    def _save_history(self):
        """Save the training history to a file."""
        with open(config.MODEL_HISTORY_PATH, 'w') as f:
            json.dump(self.history, f)
            
    def _load_history(self):
        """Load the training history from a file."""
        if os.path.exists(config.MODEL_HISTORY_PATH):
            with open(config.MODEL_HISTORY_PATH, 'r') as f:
                self.history = json.load(f)
                
    def predict_for_days(self, last_sequences, days=7):
        """
        Predict health status for the upcoming days.
        
        Args:
            last_sequences (np.ndarray): Last known sequences of vital signs
            days (int): Number of days to predict ahead
            
        Returns:
            list: Predicted health status for each day
        """
        if self.model is None:
            raise ValueError("Model must be trained before making predictions")
            
        predictions = []
        current_sequence = last_sequences[-1].copy()  # Use the most recent sequence
        
        for _ in range(days):
            # Make prediction for current sequence
            pred = self.model.predict(np.array([current_sequence]))[0][0]
            predictions.append(float(pred))
            
            # Update sequence for next prediction (rolling window)
            # Here we're simplistically using the prediction as the new health status
            # In a real scenario, you would model the temperature, SpO2, and heart rate separately
            new_step = np.append(current_sequence[-1, :-1], pred)
            current_sequence = np.vstack([current_sequence[1:], new_step])
            
        return predictions 